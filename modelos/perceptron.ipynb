{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-0Lh4DTYriKb"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train(self, inputs, outputs, alpha, epochs):\n",
        "    # Aqui incluímos os parâmetros que a função train deverá receber\n",
        "\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "    self.alpha = alpha\n",
        "    self.epochs = epochs\n",
        "    # Neste bloco definimos os pesos e bias que serão inicializados aletatoriamente entre 0 e 1\n",
        "\n",
        "    w11 = np.random.uniform(0,1)\n",
        "    w12 = np.random.uniform(0,1)\n",
        "    w21 = np.random.uniform(0,1)\n",
        "    w22 = np.random.uniform(0,1)\n",
        "\n",
        "    wh1 = np.random.uniform(0,1)\n",
        "    wh2 = np.random.uniform(0,1)\n",
        "\n",
        "    b1 = np.random.uniform(0,1)\n",
        "    b2 = np.random.uniform(0,1)\n",
        "    b3 = np.random.uniform(0,1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(len(inputs)):\n",
        "        # Execução dos cálculos lineares da camada de entrada que serão passados adiante para a camada oculta\n",
        "        # Utilizada a função de ativação sigmoid\n",
        "        h1 = 1 / (1 + np.exp(- ((inputs[i][0] * w11) + (inputs[i][0] * w21) + b1)))\n",
        "        h2 = 1 / (1 + np.exp(- ((inputs[i][0] * w12) + (inputs[i][0] * w22) + b2)))\n",
        "\n",
        "        # A partir dos valores obtidos na camada oculta, aplicamos a função de ativação novamente para assim passarmos o resultado para o neurônio de saída\n",
        "\n",
        "        y = 1 / (1 + np.exp(- ((h1 * wh1) + (h2 * wh2) + b3)))\n",
        "\n",
        "        #calculamos o erro\n",
        "        error = outputs[i][0] - y\n",
        "\n",
        "        # Nessa etapa entram alguns conceitos mais densos que envolvem o temido cálculo (conceito de derivadas parciais)\n",
        "        # Por se tratar de um assunto extenso, não entrarei em detalhes, pois esse assunto por si só gera um artigo completo (tarefa para as próximas postagens)\n",
        "        # Efetuamos o cálculo das derivadas parciais\n",
        "\n",
        "        derivative_y = y * (1 - y) * error\n",
        "        derivative_h1 = h1 * (1 - h1) * (wh1 * derivative_y)\n",
        "        derivative_h2 = h2 * (1 - h2) * (wh2 * derivative_y)\n",
        "        # Cálculo dos deltas que serão usados para atualização dos pesos\n",
        "\n",
        "        delta_w11 = alpha * derivative_h1 * inputs[i][0]\n",
        "        delta_w12 = alpha * derivative_h2 * inputs[i][0]\n",
        "        delta_w21 = alpha * derivative_h1 * inputs[i][1]\n",
        "        delta_w22 = alpha * derivative_h2 * inputs[i][1]\n",
        "\n",
        "        delta_b1 = alpha * derivative_h1\n",
        "        delta_b2 = alpha * derivative_h2\n",
        "        delta_b3 = alpha * derivative_y\n",
        "\n",
        "        delta_wh1 = alpha * derivative_y * h1\n",
        "        delta_wh2 = alpha * derivative_y * h2\n",
        "\n",
        "\n",
        "        # Atualizando os pesos e retornando os pesos obtidos após o treinamento\n",
        "        w11 += delta_w11\n",
        "        w12 += delta_w12\n",
        "        w21 += delta_w21\n",
        "        w22 += delta_w22\n",
        "\n",
        "        wh1 += delta_wh1\n",
        "        wh2 += delta_wh2\n",
        "\n",
        "        b1 += delta_b1\n",
        "        b2 += delta_b2\n",
        "        b3 += delta_b3\n",
        "\n",
        "    return w11, w12, w21, w22, wh1, wh2, b1, b2, b3\n",
        "\n",
        "  def predict(self, weigths, x1, x2):\n",
        "    hidden1 = 1 / (1 + np.exp(- ((x1 * weigths[0]) + (x2 * weigths[1]) + weigths[6])))\n",
        "    hidden2 = 1 / (1 + np.exp(- ((x1 * weigths[1]) + (x2 * weigths[2]) + weigths[7])))\n",
        "\n",
        "    # Note que nos cálculos anteriores da função sigmoid, retornávamos o valor puro, já aqui colocamos a condição necessária para retornar se o neurônio foi ativado ou não\n",
        "\n",
        "    return 1 if 1 / (1 + np.exp(- ((hidden1 * weigths[4]) + (hidden2 * weigths[5]) + weigths[8]))) >= 0.5 else 0\n",
        "\n",
        "# Dados de entrada e saída baseados na tabela verdade usando a porta lógica XOR\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs = [[0], [1], [1], [0]]\n",
        "# Executamos o treinamento da rede com uma taxa de aprendizado de 5% e 10.000 épocas de treinamento\n",
        "\n",
        "mlp = MLP()\n",
        "tr = mlp.train(inputs, outputs, 0.05, 10000)\n",
        "# Testamos o algoritmo utilizando as entradas 1 e 1, se o algoritmo foi treinado com uma taxa de aprendizado adequada e por uma quantidade razoável de épocas, iremos obter um bom resultado, mostrando que a máquina aprendeu a porta XOR\n",
        "\n",
        "y = mlp.predict(tr, 1, 1)\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQoWzmqq4oNu",
        "outputId": "f2110c5a-2925-46c8-da8a-469441fa2462"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ]
}